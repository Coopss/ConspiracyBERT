{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "   AutoConfig,\n",
    "   AutoTokenizer,\n",
    "   TFAutoModelForSequenceClassification,\n",
    "   AdamW,\n",
    "   glue_convert_examples_to_features\n",
    ")\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br /> \n",
    "model_name = 'digitalepidemiologylab/covid-twitter-bert' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
    "\n",
    "# Initialise tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 101, 7592, 2088,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('hello world')\n",
    "\n",
    "tokenizer('hello world' , return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  4074  3557 ...     0     0     0]\n",
      " [  101  2183  2067 ...     0     0     0]\n",
      " [  101  6902  2703 ...     0     0     0]\n",
      " ...\n",
      " [  101  2339  2572 ...     0     0     0]\n",
      " [  101  1030  4717 ...     0     0     0]\n",
      " [  101  3827 16067 ...     0     0     0]], shape=(10002, 328), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# # import data records\n",
    "\n",
    "for filename in os.listdir('../src/data'):\n",
    "    if 'standardized' in filename:\n",
    "        with open('../src/data/'+filename, 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            \n",
    "            \n",
    "            print(x['input_ids'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\Ryan\\tensorflow_datasets\\glue\\sst2\\1.0.0\n",
      "INFO:absl:Reusing dataset glue (C:\\Users\\Ryan\\tensorflow_datasets\\glue\\sst2\\1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from C:\\Users\\Ryan\\tensorflow_datasets\\glue\\sst2\\1.0.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from C:\\Users\\Ryan\\tensorflow_datasets\\glue\\sst2\\1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The dataset is downloaded. The entire dataset has 68221 examples of which you are using 5%. This will result in a train dataset with 3367 examples and a validation dataset with 43 examples.\n"
     ]
    }
   ],
   "source": [
    "# Paramteters\n",
    "#@markdown >Batch size and sequence length needs to be set to prepare the data. The size of the batches depends on available memory. For Colab GPU limit batch size to 8 and sequence length to 96. By reducing the length of the input (max_seq_length) you can also increase the batch size. For a dataset like SST-2 with lots of short sentences. this will likely benefit training.\n",
    "max_seq_length = 96 #@param {type: \"integer\"}\n",
    "train_batch_size =  8#@param {type: \"integer\"} \n",
    "eval_batch_size = 8 #@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "#@markdown >The Glue dataset has around 62000 examples, and we really do not need them all for training a decent model. To cut down training time, please reduse this to only a percentage of the entire set.\n",
    "use_percentage_of_data = 5 #@param {type: \"slider\", min: 1, max: 100}\n",
    "\n",
    "# get dataset sizes\n",
    "glue_builder = tfds.builder('glue/sst2')\n",
    "num_train_examples = glue_builder.info.splits['train'].num_examples\n",
    "num_dev_examples = glue_builder.info.splits['validation'].num_examples\n",
    "num_labels = glue_builder.info.features['label'].num_classes\n",
    "\n",
    "# download datasets and convert to training features\n",
    "glue_builder.download_and_prepare()\n",
    "train_data = glue_builder.as_dataset(split='train')\n",
    "train_dataset = glue_convert_examples_to_features(train_data, tokenizer, max_length=max_seq_length, task='sst-2')\n",
    "train_dataset = train_dataset.shuffle(100).batch(train_batch_size)\n",
    "\n",
    "dev_data = glue_builder.as_dataset(split='validation')\n",
    "dev_dataset = glue_convert_examples_to_features(dev_data, tokenizer, max_length=max_seq_length, task='sst-2')\n",
    "dev_dataset = dev_dataset.shuffle(100).batch(eval_batch_size)\n",
    "\n",
    "# Map the labels for printing\n",
    "label_mapping = {i: glue_builder.info.features['label'].int2str(i) for i in range(num_labels)}\n",
    "\n",
    "print(f'\\n\\nThe dataset is downloaded. The entire dataset has {num_train_examples + num_dev_examples} examples of which you are using {use_percentage_of_data}%. This will result in a train dataset with {int(num_train_examples * (use_percentage_of_data/100))} examples and a validation dataset with {int(num_dev_examples * (use_percentage_of_data/100))} examples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at digitalepidemiologylab/covid-twitter-bert were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert and are newly initialized: ['dropout_147', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/420 [=======>......................] - ETA: 9:56 - loss: 0.4562 - accuracy: 0.7939"
     ]
    }
   ],
   "source": [
    "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
    "learning_rate = 2e-5 #@param {type: \"number\"}\n",
    "\n",
    "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
    "num_epochs = 1  #@param {type: \"integer\"}\n",
    "\n",
    "# Initialise a Model for Sequence Classification with 2 labels\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Metrics and callbacks\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "checkpoint_path = './checkpoints/checkpoint.{epoch:02d}'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
    "\n",
    "# Compute some variables\n",
    "train_steps_per_epoch = int(num_train_examples * (use_percentage_of_data/100) / train_batch_size)\n",
    "dev_steps_per_epoch = int(num_dev_examples * (use_percentage_of_data/100) / eval_batch_size)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, \n",
    "  epochs=num_epochs,\n",
    "  steps_per_epoch=train_steps_per_epoch,\n",
    "  validation_data=dev_dataset,\n",
    "  validation_steps=dev_steps_per_epoch,\n",
    "  callbacks=callbacks)\n",
    "\n",
    "# Print some information about the training\n",
    "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
    "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
    "print(json.dumps(history.history, indent=4))\n",
    "\n",
    "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
    "!ls -lha ./checkpoints/\n",
    "\n",
    "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
    "model.save_pretrained('./huggingface_model/')\n",
    "\n",
    "print('\\nTensorflow model and config-file is saved in ./huggingface_model/')\n",
    "!ls -lha ./huggingface_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
