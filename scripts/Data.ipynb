{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# from auth import username, password\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pickle\n",
    "import os\n",
    "import preprocessor as p\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'christian'\n",
    "password = 'Dec211996'\n",
    "\n",
    "# setup pickle data dir\n",
    "data_dir = 'data'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "client = MongoClient('mongodb://' + urllib.parse.quote_plus(username) + ':' + urllib.parse.quote_plus(password) + '@198.211.115.252')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def worker(consp_tuple):\n",
    "    data = []\n",
    "    conspiracy, hashtag = consp_tuple\n",
    "    cursor = client[conspiracy][hashtag].find({})\n",
    "    for i, document in enumerate(cursor):\n",
    "        try:\n",
    "            inputs = tokenizer(p.clean(document['text']) + ' [SEP]', return_tensors=\"tf\")\n",
    "            inputs.update({'tweetId' : document['tweetId']})\n",
    "            data.append(inputs)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "        if (i > 10000):\n",
    "            break\n",
    "\n",
    "    with open(data_dir + '/' + conspiracy + '-' + hashtag + '.p', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return consp_tuple\n",
    "\n",
    "def get_consp_tuples():\n",
    "    ignore_mask = ['test_db', 'admin', 'local', 'config', 'TwitterJobs']\n",
    "    conspiracies = list(set(client.list_database_names()) - set(ignore_mask))\n",
    "\n",
    "    consp_tuples = []\n",
    "    for conspiracy in conspiracies:\n",
    "        for hashtag in client[conspiracy].list_collection_names():\n",
    "            consp_tuples.append((conspiracy, hashtag))\n",
    "\n",
    "    return consp_tuples\n",
    "\n",
    "def preprocess():\n",
    "    consp_tuples = get_consp_tuples()\n",
    "\n",
    "    p = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "    results = p.map(worker, consp_tuples)\n",
    "\n",
    "    return results\n",
    "\n",
    "def standardize_tensor_shape():\n",
    "    consp_tuples = get_consp_tuples()\n",
    "\n",
    "    global_max = 0\n",
    "    for consp_tuple in tqdm(consp_tuples):\n",
    "        conspiracy, hashtag = consp_tuple\n",
    "        try:\n",
    "            with open(data_dir + '/' + conspiracy + '-' + hashtag + '.p', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "                max_size = max(data, key= lambda x: x['input_ids'].shape[1])['input_ids'].shape[1]\n",
    "                if max_size > global_max:\n",
    "                    global_max = max_size\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Could not find: \" + data_dir + '/' + conspiracy + '-' + hashtag + '.p')\n",
    "            pass\n",
    "\n",
    "    print(\"global_max: \" + str(global_max))\n",
    "\n",
    "    for consp_tuple in tqdm(consp_tuples):\n",
    "        conspiracy, hashtag = consp_tuple\n",
    "\n",
    "        try:\n",
    "            with open(data_dir + '/' + conspiracy + '-' + hashtag + '.p', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "                input_ids = tf.zeros((0, global_max), dtype=tf.int32)\n",
    "                token_type_ids = tf.zeros((0, global_max), dtype=tf.int32)\n",
    "                attention_mask = tf.zeros((0, global_max), dtype=tf.int32)\n",
    "\n",
    "                for row in data:\n",
    "                    row_input_ids = tf.concat([row['input_ids'], tf.zeros((1,global_max-row['input_ids'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "                    row_token_type_ids = tf.concat([row['token_type_ids'], tf.zeros((1,global_max-row['token_type_ids'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "                    row_attention_mask = tf.concat([row['attention_mask'], tf.zeros((1,global_max-row['attention_mask'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "\n",
    "                    input_ids = tf.concat([input_ids, row_input_ids], axis=0)\n",
    "                    token_type_ids = tf.concat([token_type_ids, row_token_type_ids], axis=0)\n",
    "                    attention_mask = tf.concat([attention_mask, row_attention_mask], axis=0)\n",
    "\n",
    "                conspiracy_x = {'input_ids':input_ids, 'token_type_ids':token_type_ids, 'attention_mask':attention_mask}\n",
    "\n",
    "            with open(data_dir + '/' + conspiracy + '-' + hashtag + '-standardized.p', 'wb') as f:\n",
    "                pickle.dump(conspiracy_x, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Could not find: \" + data_dir + '/' + conspiracy + '-' + hashtag + '.p')\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for row in data:\n",
    "    #     row['input_ids'] = tf.concat([row['input_ids'], tf.zeros((1,max_size-row['input_ids'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "    #     row['token_type_ids'] = tf.concat([row['token_type_ids'], tf.zeros((1,max_size-row['token_type_ids'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "    #     row['attention_mask'] = tf.concat([row['attention_mask'], tf.zeros((1,max_size-row['attention_mask'].shape[1]), dtype=tf.int32)], axis=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(preprocess())\n",
    "    standardize_tensor_shape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
